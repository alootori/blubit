from transformers import pipeline, AutoTokenizer

# Load the tokenizer and explicitly set the pad token to be the eos token
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-3b-instruct")
tokenizer.pad_token = tokenizer.eos_token

# Set up the pipeline with the explicit pad_token_id parameter
llama_pipeline = pipeline(
    "text-generation",
    model="meta-llama/Llama-3-3b-instruct",
    tokenizer=tokenizer,
    device_map="auto",  # Automatically allocates model to GPU if available
    pad_token_id=tokenizer.eos_token_id
)
