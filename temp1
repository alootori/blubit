import pandas as pd
import re
import spacy
import matplotlib.pyplot as plt

# --- Step 1: Load and Explore the Data ---
# Adjust the file path as needed.
csv_path = "path/to/your_file.csv"
df = pd.read_csv(csv_path)

# Basic exploration
print("Data Shape:", df.shape)
print("Columns:", df.columns.tolist())
print("First 5 rows:")
print(df.head())
print("\nData info:")
print(df.info())

# --- Step 2: Preprocess and Clean the Text ---
# Fill missing text values, convert to lowercase, and remove extra whitespace.
df['text_clean'] = df['text'].fillna("").apply(lambda x: re.sub(r'\s+', ' ', x.lower()).strip())

# Optionally, examine the distribution of text lengths
df['text_length'] = df['text_clean'].apply(len)
print("\nText Length Statistics:")
print(df['text_length'].describe())

# Plot a histogram of text lengths (using a sample for performance reasons)
sample_df = df.sample(n=100000, random_state=42)  # adjust sample size as needed
plt.hist(sample_df['text_length'], bins=50, color='skyblue', edgecolor='black')
plt.title("Distribution of Text Lengths")
plt.xlabel("Text Length (characters)")
plt.ylabel("Frequency")
plt.show()

# --- Step 3: Sentence Segmentation ---
# Load spaCy English model (you may need to download it first with: python -m spacy download en_core_web_sm)
nlp = spacy.load("en_core_web_sm")

def split_into_sentences(text):
    """Splits text into sentences using spaCy."""
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents]

# Apply sentence segmentation; this creates a new column with a list of sentences.
df['sentences'] = df['text_clean'].apply(split_into_sentences)

# For a quick check, print the sentences from the first few rows.
for idx, row in df.head(5).iterrows():
    print(f"\nID: {row['id']}")
    print("Sentences:")
    for sent in row['sentences']:
        print(f" - {sent}")

# --- Step 4: Basic Keyword Frequency Analysis ---
keywords = ["called", "requested", "withdraw", "close"]
for keyword in keywords:
    col_name = f'contains_{keyword}'
    df[col_name] = df['text_clean'].apply(lambda x: keyword in x)
    count = df[col_name].sum()
    print(f"\nKeyword '{keyword}' frequency: {count}")

# Optional: Save the processed DataFrame for later steps.
df.to_csv("processed_data.csv", index=False)
