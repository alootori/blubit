import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Define Model Path (Ensure the downloaded files are inside this folder)
model_path = "phi-1.5"  # Adjust if stored elsewhere

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load Model with `safetensors`
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map="auto")

# Move Model to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print(f" Model successfully loaded on: {device}")

# ----------------------------
# Function to Query the Model
# ----------------------------

def query_text(text):
    """Asks the model if the input text mentions someone calling to withdraw."""
    prompt = f"""You are an AI assistant. Read the following text and answer:
    
    TEXT: "{text}"

    QUESTION: Does this text mention someone calling to withdraw something? Answer Yes or No."""

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(**inputs, max_new_tokens=10)

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response

#  Example Input Text
input_text = "The customer called and requested to withdraw their request."

#  Query Model
result = query_text(input_text)
print("Model Response:", result)
