import re
import pandas as pd
from transformers import pipeline

# --- Set up your Llama 3.2 instruct model pipeline ---
llama_pipeline = pipeline("text-generation", model="path/to/llama-3.2-instruct", max_length=256)

def classify_call_note_with_llama(text):
    """
    Uses the Llama model to classify a call note.
    It asks whether the note indicates that the customer actively requested
    to withdraw or close their claim, and if 'Yes', to extract the supporting sentence.
    """
    prompt = (
        "Does the following customer call note indicate that the customer actively requested "
        "to withdraw or close their claim? Answer 'Yes' or 'No'. If 'Yes', also extract the sentence "
        "that best supports this decision.\n\n"
        "Call Note: \"\"\"\n" + text + "\n\"\"\"\n\nAnswer:"
    )
    # Get the generated answer from the model
    output = llama_pipeline(prompt, max_length=150, do_sample=False)[0]['generated_text']
    
    # Simple parsing: if the output contains 'yes', we mark it as positive.
    if "yes" in output.lower():
        # Try to split on a colon to extract the relevant sentence.
        parts = output.split(":")
        decision = "Yes"
        relevant_sentence = parts[1].strip() if len(parts) > 1 else ""
    else:
        decision = "No"
        relevant_sentence = ""
    
    # Confidence is not directly provided; set to None (or add your own heuristic here).
    confidence = None
    return decision, relevant_sentence, confidence, output

def split_text_into_chunks(text, max_chunk_size=2000):
    """
    Splits a long text into chunks of approximately max_chunk_size characters,
    trying to preserve sentence boundaries.
    """
    # Regex splits on punctuation followed by whitespace/newlines.
    pattern = re.compile(r'(?<=[.!?])(?:\s+|[\r\n]+)+')
    sentences = pattern.split(text.strip())
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        # If adding the sentence would exceed the max_chunk_size, start a new chunk.
        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
            current_chunk = current_chunk + " " + sentence if current_chunk else sentence
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def process_call_note(text, max_chunk_size=2000):
    """
    Processes a call note. If the text length exceeds max_chunk_size,
    it splits the note into chunks and processes each one.
    If any chunk is classified as 'Yes', it returns that decision along with the
    extracted relevant sentence (and the combined model output).
    """
    # If the text is short enough, process it directly.
    if len(text) <= max_chunk_size:
        return classify_call_note_with_llama(text)
    
    # Otherwise, split the text into manageable chunks.
    chunks = split_text_into_chunks(text, max_chunk_size)
    overall_decision = "No"
    overall_relevant_sentence = ""
    overall_confidence = None
    overall_full_output = ""
    
    for chunk in chunks:
        decision, relevant_sentence, confidence, full_output = classify_call_note_with_llama(chunk)
        overall_full_output += f"\nChunk Output:\n{full_output}\n"
        if decision.lower() == "yes":
            overall_decision = "Yes"
            overall_relevant_sentence = relevant_sentence
            overall_confidence = confidence
            # Optionally, break as soon as a positive result is found.
            break
            
    return overall_decision, overall_relevant_sentence, overall_confidence, overall_full_output

def process_dataframe(df, text_col='text', max_chunk_size=2000):
    """
    Applies the call note processing function to a DataFrame column.
    The results are stored in new columns:
      - llama_decision
      - llama_relevant_sentence
      - llama_confidence
      - llama_full_output
    """
    results = df[text_col].apply(lambda x: process_call_note(x, max_chunk_size))
    df['llama_decision'] = results.apply(lambda x: x[0])
    df['llama_relevant_sentence'] = results.apply(lambda x: x[1])
    df['llama_confidence'] = results.apply(lambda x: x[2])
    df['llama_full_output'] = results.apply(lambda x: x[3])
    return df

# --- Example usage ---
# Assuming your DataFrame is loaded in variable `df` and has a text column (e.g., 'text')
# If you have already preprocessed the text (like lowercasing and whitespace normalization),
# you can use that column; here we use 'text' directly.
processed_df = process_dataframe(df, text_col='text', max_chunk_size=2000)

# Save the results to a new CSV file
processed_df.to_csv("df_with_llama_results.csv", index=False)
